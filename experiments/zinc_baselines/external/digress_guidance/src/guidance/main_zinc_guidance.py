"""
Guidance sampling for ZINC dataset using trained property regressor.

Usage:
    python -m src.guidance.main_zinc_guidance experiment=zinc_guidance \\
        general.guidance_target=qed \\
        general.trained_regressor_path=checkpoints/zinc_regressor/last.ckpt \\
        general.test_only=checkpoints/zinc_unconditional/last.ckpt
"""
import os
os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'

from rdkit import Chem
import torch
import wandb
import hydra
import omegaconf
from omegaconf import DictConfig, OmegaConf, open_dict
from pytorch_lightning import Trainer
from pytorch_lightning.utilities.warnings import PossibleUserWarning
import warnings

import src.utils as utils
from src.guidance.guidance_diffusion_model_discrete import DiscreteDenoisingDiffusion
from src.datasets.zinc_dataset import ZINCDataModule, ZINCinfos, get_train_smiles
from src.metrics.molecular_metrics import SamplingMolecularMetrics, TrainMolecularMetricsDiscrete
from src.analysis.visualization import MolecularVisualization
from src.diffusion.extra_features import DummyExtraFeatures, ExtraFeatures
from src.diffusion.extra_features_molecular import ExtraMolecularFeatures
from src.utils import update_config_with_new_keys
from src.guidance.zinc_regressor_discrete import ZincRegressorDiscrete


warnings.filterwarnings("ignore", category=PossibleUserWarning)


def get_resume(cfg, model_kwargs):
    """Load pretrained unconditional diffusion model."""
    saved_cfg = cfg.copy()

    name = cfg.general.name + '_resume'
    resume = cfg.general.test_only
    final_samples_to_generate = cfg.general.final_model_samples_to_generate
    final_chains_to_save = cfg.general.final_model_chains_to_save
    batch_size = cfg.train.batch_size
    
    model = DiscreteDenoisingDiffusion.load_from_checkpoint(resume, **model_kwargs)
    cfg = model.cfg
    cfg.general.test_only = resume
    cfg.general.name = name
    cfg.general.final_model_samples_to_generate = final_samples_to_generate
    cfg.general.final_model_chains_to_save = final_chains_to_save
    cfg.train.batch_size = batch_size
    cfg = update_config_with_new_keys(cfg, saved_cfg)
    return cfg, model


def setup_wandb(cfg):
    config_dict = omegaconf.OmegaConf.to_container(cfg, resolve=True, throw_on_missing=True)
    kwargs = {
        'name': cfg.general.name, 
        'project': 'zinc_guidance', 
        'config': config_dict,
        'settings': wandb.Settings(_disable_stats=True),
        'reinit': True, 
        'mode': cfg.general.wandb
    }
    wandb.init(**kwargs)
    wandb.save('*.txt')
    return cfg


@hydra.main(version_base='1.1', config_path='../../configs', config_name='config')
def main(cfg: DictConfig):
    dataset_config = cfg["dataset"]
    assert dataset_config.name == "zinc", "This script is for ZINC dataset"
    
    # Initialize datamodule
    datamodule = ZINCDataModule(cfg, regressor=True)
    dataset_infos = ZINCinfos(datamodule=datamodule, cfg=cfg)
    datamodule.prepare_data()
    train_smiles = get_train_smiles(cfg, datamodule, dataset_infos)

    # Extra features
    if cfg.model.extra_features is not None:
        extra_features = ExtraFeatures(cfg.model.extra_features, dataset_info=dataset_infos)
        domain_features = ExtraMolecularFeatures(dataset_infos=dataset_infos)
    else:
        extra_features = DummyExtraFeatures()
        domain_features = DummyExtraFeatures()

    dataset_infos.compute_input_output_dims(
        datamodule=datamodule, 
        extra_features=extra_features,
        domain_features=domain_features
    )

    train_metrics = TrainMolecularMetricsDiscrete(dataset_infos)
    sampling_metrics = SamplingMolecularMetrics(dataset_infos, train_smiles)
    visualization_tools = MolecularVisualization(remove_h=True, dataset_infos=dataset_infos)

    model_kwargs = {
        'dataset_infos': dataset_infos, 
        'train_metrics': train_metrics,
        'sampling_metrics': sampling_metrics, 
        'visualization_tools': visualization_tools,
        'extra_features': extra_features, 
        'domain_features': domain_features, 
        'load_model': True
    }

    # Load pretrained unconditional model
    cfg_pretrained, guidance_sampling_model = get_resume(cfg, model_kwargs)

    OmegaConf.set_struct(cfg, True)
    with open_dict(cfg):
        cfg.model = cfg_pretrained.model
    model_kwargs['load_model'] = False

    utils.create_folders(cfg)
    cfg = setup_wandb(cfg)

    # Load pretrained regressor
    regressor_path = cfg.general.trained_regressor_path
    print(f"Loading regressor from {regressor_path}")
    guidance_model = ZincRegressorDiscrete.load_from_checkpoint(regressor_path)

    model_kwargs['guidance_model'] = guidance_model

    # Trainer for testing/sampling
    trainer = Trainer(
        gradient_clip_val=cfg.train.clip_grad,
        accelerator='gpu' if cfg.general.gpus > 0 and torch.cuda.is_available() else 'cpu',
        devices=1 if cfg.general.gpus > 0 and torch.cuda.is_available() else None,
        limit_test_batches=100,
        max_epochs=cfg.train.n_epochs,
        check_val_every_n_epoch=cfg.general.check_val_every_n_epochs,
        fast_dev_run=cfg.general.name == 'debug',
        enable_progress_bar=True,
        logger=[],
    )

    # Setup model for conditional sampling
    model = guidance_sampling_model
    model.args = cfg
    model.guidance_model = guidance_model
    
    print(f"\nStarting guided sampling with target: {cfg.general.guidance_target}")
    print(f"Guidance strength (lambda): {cfg.guidance.lambda_guidance}")
    print(f"Generating {cfg.general.final_model_samples_to_generate} samples...")
    
    trainer.test(model, datamodule=datamodule, ckpt_path=None)


if __name__ == '__main__':
    main()
